{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, scale\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "array1 = np.random.normal(loc = 50, scale=5, size=100)\n",
    "array2 = np.random.normal(loc = 60, scale=2, size=100)\n",
    "array3 = np.vstack((array1, array2)).T\n",
    "a3_df = pd.DataFrame(array3)\n",
    "StdScaler = StandardScaler()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General Reasons for Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Are required by some machine learning algorithms\n",
    "2) \"Unscaled data can also slow down or even prevent the convergence of many gradient-based estimators.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithms that Do Not Require Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Linear regression (without gradient descent)\n",
    "2) Logistic regression (without gradient descent)\n",
    "3) Decision Trees and Random Forests\n",
    "4) Naive Bayes (?)\n",
    "5) Linear Discriminant Analysis(LDA) (?)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General Information About Scalers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) \"Scalers are linear (or more precisely affine) transformers\"\n",
    "2) Can intake arrays, series and dataframes, but return arrays\n",
    "3) Have the same set of methods, but different parameters (i.e., hyperparameters) and attributes (parameters)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standard Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "a1_scale = scale(array1)\n",
    "a2_scale = scale(array2)\n",
    "a3_scale = scale(array3)\n",
    "a3_df_scale = scale(a3_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.89588916,  1.26027338,  2.13964297,  0.7563316 , -1.51852875])"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a1_scale[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.89588916, -0.4626879 ],\n",
       "       [ 1.26027338, -1.18378559],\n",
       "       [ 2.13964297,  1.00225161],\n",
       "       [ 0.7563316 , -0.88672299],\n",
       "       [-1.51852875, -0.34785406]])"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a3_df_scale[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 100\n"
     ]
    }
   ],
   "source": [
    "a3_scale_df = pd.DataFrame(a3_scale)\n",
    "print((a3_scale_df[0]==a1_scale).sum(), (a3_df[1]==a2_scale).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1.334488e-15\n",
       "1   -1.273426e-15\n",
       "dtype: float64"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(a3_scale_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1.000000e+02</td>\n",
       "      <td>1.000000e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.298961e-15</td>\n",
       "      <td>-1.285638e-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.005038e+00</td>\n",
       "      <td>1.005038e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-2.463903e+00</td>\n",
       "      <td>-3.147010e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-7.011991e-01</td>\n",
       "      <td>-6.606540e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-7.981316e-02</td>\n",
       "      <td>1.445285e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>5.449091e-01</td>\n",
       "      <td>7.059845e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2.979439e+00</td>\n",
       "      <td>2.102990e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  0             1\n",
       "count  1.000000e+02  1.000000e+02\n",
       "mean   1.298961e-15 -1.285638e-15\n",
       "std    1.005038e+00  1.005038e+00\n",
       "min   -2.463903e+00 -3.147010e+00\n",
       "25%   -7.011991e-01 -6.606540e-01\n",
       "50%   -7.981316e-02  1.445285e-01\n",
       "75%    5.449091e-01  7.059845e-01\n",
       "max    2.979439e+00  2.102990e+00"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a3_scale_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.298960938811433e-15"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(a3_df[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-2.68170191e-01,  1.37658231e+00],\n",
       "       [-1.27488934e+00, -6.95114201e-01],\n",
       "       [ 3.91416929e-01, -8.57022221e-01],\n",
       "       [ 7.83989317e-01,  1.60713470e+00],\n",
       "       [-6.89619362e-01, -3.54620647e-01],\n",
       "       [ 8.60610042e-01,  1.93253029e+00],\n",
       "       [-4.87320045e-01, -5.58929062e-01],\n",
       "       [ 2.18364114e+00,  1.64232537e-01],\n",
       "       [ 2.35583592e-01,  1.05566701e+00],\n",
       "       [ 1.40533843e-01,  4.14650467e-01],\n",
       "       [ 8.42404770e-01, -1.22687306e+00],\n",
       "       [ 1.67961608e+00, -5.77775940e-01],\n",
       "       [-7.79162968e-01,  1.86128589e+00],\n",
       "       [ 2.97943922e+00,  1.30590575e+00],\n",
       "       [-1.24710978e-01,  8.38522445e-01],\n",
       "       [ 4.56170863e-01,  1.06683687e+00],\n",
       "       [ 3.25440553e-01, -1.45881187e+00],\n",
       "       [ 1.36156467e+00,  2.10299048e+00],\n",
       "       [-8.87182608e-01,  2.54005505e-01],\n",
       "       [-1.57016615e-01, -1.90947256e+00],\n",
       "       [ 5.79936361e-01,  2.92045473e-01],\n",
       "       [-7.78011194e-01, -1.69757731e+00],\n",
       "       [-1.50633170e+00,  7.87753486e-01],\n",
       "       [-1.45492778e+00, -4.48365130e-01],\n",
       "       [-8.74748814e-01,  3.28063087e-01],\n",
       "       [-3.90364480e-01, -1.24394915e-01],\n",
       "       [ 1.00021823e+00,  2.84502286e-01],\n",
       "       [-3.96771516e-01,  1.30728419e+00],\n",
       "       [ 5.11198417e-01, -1.12950404e+00],\n",
       "       [ 2.99177803e-01, -5.81479892e-01],\n",
       "       [-4.81160045e-01,  1.70170462e+00],\n",
       "       [-1.81321173e+00, -1.79803823e-01],\n",
       "       [-2.47450983e-01,  1.47698766e-01],\n",
       "       [ 2.50683289e+00, -2.00263340e-01],\n",
       "       [ 6.40703588e-01, -4.17364391e-01],\n",
       "       [-9.57449633e-01, -1.84616714e+00],\n",
       "       [ 1.49320628e+00, -1.59987254e+00],\n",
       "       [ 8.01764607e-01,  8.85938169e-01],\n",
       "       [-4.07905249e-01,  1.58997629e-01],\n",
       "       [ 1.59675643e-01, -7.59531358e-01],\n",
       "       [ 4.77428765e-01,  1.21987161e+00],\n",
       "       [ 7.49315873e-01, -6.93674338e-01],\n",
       "       [ 9.96274565e-01,  5.33696448e-01],\n",
       "       [-8.62071233e-01, -1.05980013e+00],\n",
       "       [ 5.18101903e-01,  1.19990468e-01],\n",
       "       [-3.58779333e-01,  1.30521188e+00],\n",
       "       [ 8.52800764e-01,  4.67343254e-01],\n",
       "       [-1.36246696e-01, -1.07456871e+00],\n",
       "       [-1.43285948e+00,  5.64225168e-01],\n",
       "       [-2.46390343e+00,  7.42422889e-01],\n",
       "       [ 1.61054336e+00, -4.45038100e-01],\n",
       "       [-8.13240542e-01, -8.56409032e-01],\n",
       "       [-7.41819029e-03, -1.22744691e+00],\n",
       "       [-6.89874043e-01, -4.57159224e-02],\n",
       "       [-1.11928818e+00,  2.14606294e-01],\n",
       "       [ 1.99405678e-01,  2.43114866e-01],\n",
       "       [ 1.89158836e-01,  4.55321130e-01],\n",
       "       [ 8.82619510e-01, -3.64346008e-01],\n",
       "       [-1.05428285e+00, -8.48893410e-01],\n",
       "       [-1.91888504e-01,  1.98836824e-01],\n",
       "       [ 2.10463341e-01,  7.04073467e-01],\n",
       "       [-6.31163276e-01,  2.67556324e-01],\n",
       "       [ 6.96572222e-02,  2.30312387e-01],\n",
       "       [ 1.80497983e+00,  2.61578454e-01],\n",
       "       [-8.28832690e-01, -1.55416613e+00],\n",
       "       [ 9.64037862e-02,  7.11717775e-01],\n",
       "       [-5.72458082e-01, -1.40704427e+00],\n",
       "       [-8.34848905e-01, -7.20101671e-01],\n",
       "       [-2.28178261e-01,  1.38118591e+00],\n",
       "       [ 9.51921609e-01, -1.64633669e+00],\n",
       "       [-1.17147189e+00, -7.92258463e-02],\n",
       "       [-8.14213284e-01, -7.16650793e-01],\n",
       "       [-6.40410616e-01,  7.99612082e-01],\n",
       "       [-7.44466428e-01, -4.52495477e-01],\n",
       "       [-2.39137208e-01, -1.87333549e+00],\n",
       "       [-3.10621043e-02,  1.57312261e+00],\n",
       "       [-1.02265183e-03,  8.23820649e-02],\n",
       "       [ 6.32986408e-02, -9.89214963e-01],\n",
       "       [-7.35174113e-01, -4.44464123e-01],\n",
       "       [ 1.38743487e+00,  2.62913397e-01],\n",
       "       [ 2.54624306e-01,  8.58706971e-01],\n",
       "       [ 2.26259734e+00, -2.83608031e-02],\n",
       "       [-1.29076667e+00, -6.91630130e-02],\n",
       "       [-3.49153440e-02,  4.41859048e-01],\n",
       "       [-1.38101511e+00,  3.16030083e-03],\n",
       "       [ 6.59023032e-02,  1.52490297e-01],\n",
       "       [-1.73394539e+00, -6.49647189e-01],\n",
       "       [-1.85625797e-01,  1.26883089e+00],\n",
       "       [-2.40440112e-01, -1.68947794e+00],\n",
       "       [-2.63520433e-01,  1.49567190e+00],\n",
       "       [ 5.33233410e-01,  3.70083351e-01],\n",
       "       [-4.70341394e-01, -3.14700965e+00],\n",
       "       [-1.84073755e+00,  5.11072088e-01],\n",
       "       [-5.89809855e-01,  1.25142605e+00],\n",
       "       [ 1.20122278e+00,  8.49196489e-01],\n",
       "       [ 2.12516334e-01, -4.90545018e-01],\n",
       "       [-4.56223820e-01, -6.11306974e-01],\n",
       "       [ 9.12381485e-01,  4.58157411e-01],\n",
       "       [ 4.20630908e-01,  1.41358232e-01],\n",
       "       [ 1.90999642e+00, -2.08058255e-01]])"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a3_Std = StdScaler.fit_transform(array3)\n",
    "a3_Std\n",
    "# a3_Std_df = pd.DataFrame(a3_Std)\n",
    "# print(np.mean(a3_Std_df[0]), np.mean(a3_Std_df[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "a1_StdScale = StdScaler.fit_transform(array1.reshape(-1,1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.2856382625159313e-15"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a2_StdScale = StdScaler.fit_transform(array2.reshape(-1,1))\n",
    "np.mean(a2_StdScale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scale will work on a 1D-array, StandardScaler requires a 2D-array or requires that a single feature to be reshaped as array.reshape(-1, 1). With StandardScaler it is necessary to fit and transform.  \n",
    "\n",
    "The outputs of the two methods are different. Calling scale gives a transformed array. Calling StandardScaler gives a ___ object. Fit and transform are needed to get the data. \n",
    "\n",
    "The StandardScaler can apply a transformation fit on one dataset to a second dataset. \n",
    "\n",
    "In both, \"NaNs are treated as missing values: disregarded in fit, and maintained in transform.\"\n",
    "\n",
    "In both, \"We use a biased estimator for the standard deviation, equivalent to numpy.std(x, ddof=0).\"\n",
    "\n",
    "*Compare parameters and attributes of each method\n",
    "\n",
    "StandardScaler has methods (fit, fit_transform, get_params, inverse_transform, partial_fit, set_params, and transform)\n",
    "\n",
    "Scale is a function. StandardScaler is an utility class that creates an object.\n",
    "\n",
    "Both can be applied to multi-dimensional arrays and pandas dataframes containing multiple features.\n",
    "\n",
    "np.mean and .mean(), when called on an entire dataframe give inaccurate results for very small floats. However, calling on individual columns gives the correct result. \n",
    "\n",
    "Use Cases: \n",
    "1) Scale and StandardScaler are used with the RBF kernel in SVMs and L1 and L2 regularizers of linear models (i.e., Lasso and Ridge regression).\n",
    "2) Metric-based and gradient-based estimators\n",
    "3) PCA\n",
    "4) Perceptrons\n",
    "5) Neural Networks\n",
    "6) Algorithms that use Euclidian distance (e.g., KNN, k-means, HAC)\n",
    "7) Logistic regression using gradient descent\n",
    "\n",
    "\n",
    "\n",
    "\"This scaler can also be applied to sparse CSR or CSC matrices by passing with_mean=False to avoid breaking the sparsity structure of the data.\"\n",
    "\n",
    "Very sensitive to outliers\n",
    "\n",
    "Uses a biased estimator of the standard deviation (i.e., sets degrees of freedom to 0). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MaxAbsScaler and maxabs_scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Used with very small standard deviations of features and to preserve zero entries in sparse data.\n",
    "\n",
    "Used with data centered at zero or sparse data\n",
    "\n",
    "maxabs_scale is a function, while MaxAbsScaler is a class that creates an object. \n",
    "\n",
    "scales between \\[-1, 1\\]\n",
    "\n",
    "divides the values by the maximum absolute value in each feature\n",
    "\n",
    "Can be applied to sparse CSR (compressed sparse rows) or CSC (compressed sparse columns) matrices\n",
    "\n",
    "\"Suffers from the presence of large outliers\"\n",
    "\n",
    "In both, \"NaNs are treated as missing values: disregarded in fit, and maintained in transform.\" \n",
    "\n",
    "Use Cases:\n",
    "1) The recommended way to scale sparse data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MinMaxScaler and minmax_scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scales features to a given range.\n",
    "\n",
    "Scaled values often fall between \\[0, 1\\], though different min and max values can be specified.\n",
    "\n",
    "Robust to very small std deviations and preserve zeros in sparse data.\n",
    "\n",
    "Reduces the standard deviation\n",
    "\n",
    "Very sensitive to outliers.\n",
    "\n",
    "Used as an alternative to zero mean, unit variance scaling.\n",
    "\n",
    "Transformation given by:\n",
    "X_std = (X - X.min(axis=0)) / (X.max(axis=0) - X.min(axis=0))\n",
    "X_scaled = X_std * (max - min) + min\n",
    "where min and max are hyperparameters\n",
    "\n",
    "Transformation calculated by:\n",
    "X_scaled = scale * X + min - X.min(axis=0) * scale\n",
    "where scale = (max - min) / (X.max(axis=0) - X.min(axis=0))\n",
    "\n",
    "\n",
    "For MinMaxScaler, \"NaNs are treated as missing values: disregarded in fit, and maintained in transform.\"\n",
    "\n",
    "Use Cases:\n",
    "1) Data with hard boundaries such as colors in image data (https://medium.com/greyatom/why-how-and-when-to-scale-your-features-4b30ab09db5e) \n",
    "2) Some neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, minmax_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.035821816042655816, array([0.03582182]))"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a1_MM_01 = MinMaxScaler().fit(array1.reshape(-1,1))\n",
    "1/(array1.max() - array1.min()), a1_MM_01.scale_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.035821816042655816, array([0.07164363]))"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a1_MM_02 = MinMaxScaler(feature_range=(0,2)).fit(array1.reshape(-1,1))\n",
    "1/(array1.max() - array1.min()), a1_MM_02.scale_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-2.67046568]),\n",
       " array([37.2742922]),\n",
       " array([65.19024149]),\n",
       " array([27.91594929]))"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a1_MM_02.min_, a1_MM_02.data_min_, a1_MM_02.data_max_, a1_MM_02.data_range_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RobustScaler and robust_scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Robust to outliers\n",
    "\n",
    "Centers the data using the median instead of the mean.\n",
    "\n",
    "Scales (divides by) the data using a quantile range (the default is IQR). \n",
    "\n",
    "Use in machine learning tasks where the StandardScaler is typically used, but the data contains outliers.\n",
    "\n",
    "Outliers remain in the transformed data\n",
    "\n",
    "Cannot be used with sparse data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler, robust_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'sklearn.preprocessing.data.RobustScaler'>\n",
      "[50.46710884] [6.86258537]\n",
      "{'copy': True, 'quantile_range': (25.0, 75.0), 'with_centering': True, 'with_scaling': True}\n",
      "{'copy': True, 'quantile_range': (80.0, 90.0), 'with_centering': True, 'with_scaling': True}\n"
     ]
    }
   ],
   "source": [
    "a1_RS = RobustScaler(with_centering=True).fit(array1.reshape(-1,1))\n",
    "print(type(a1_RS))\n",
    "print(a1_RS.center_, a1_RS.scale_)\n",
    "print(a1_RS.get_params())\n",
    "#a1_RS.transform(array1.reshape(-1,1))\n",
    "\n",
    "a1_RS.set_params(quantile_range= (80.0, 90.0))\n",
    "print(a1_RS.get_params())\n",
    "# a1_RS = a1_RS.fit_transform(array1.reshape(-1,1))\n",
    "# print(type(a1_RS))\n",
    "#a1_RS.fit(array1.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'copy': True,\n",
       " 'quantile_range': (80.0, 90.0),\n",
       " 'with_centering': True,\n",
       " 'with_scaling': True}"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a1_RS.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fit: fits the scaler (e.g., calculates the center and scale to be used) and stores the parameters to be used to transform the data.  Returns an object. \n",
    "\n",
    "fit_transform: Combines the fit and transform methods. Returns an array of transformed values.\n",
    "\n",
    "get_params: call with or without (). The call with () returns a dictionary of the parameters (hyperparameters) of the function.\n",
    "\n",
    "inverse_transform: reverses the transformation \n",
    "\n",
    "partial_fit: Online computation of data parameters. \"All of X is processed as a single batch.\" Use with very large samples or continuously streaming data. \n",
    "\n",
    "set_params: Sets the parameters (hyperparameters) of the function. If the parameters are change after fitting the scaler, you will need to fit the scaler again to apply the changes and then transform the data. \n",
    "\n",
    "transform: Using the parameters from the fit method above, transform transforms the data. Returns an array of transformed values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ADD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean Normalization\n",
    "Normalization\n",
    "Unit Vector Scaling\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
